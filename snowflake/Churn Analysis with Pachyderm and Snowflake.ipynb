{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d88a8f-60db-492a-b7bb-600d8ed8d432",
   "metadata": {},
   "source": [
    "# Churn Analysis with Pachyderm and Snowflake\n",
    "\n",
    "<img src=\"images/kkbox_dag.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "[Predicting churn](https://www.investopedia.com/terms/c/churnrate.asp) is a task to determine whether a user will renew their subscription or not to a particular service. \n",
    "\n",
    "In this example, we use the [KKBox Churn Prediction challenge dataset](https://www.kaggle.com/competitions/kkbox-churn-prediction-challenge/data) from [Kaggle](https://www.kaggle.com/) to show a real world setup for predicting churn for a music service. To this end, we use [Snowflake](https://www.snowflake.com/) as our data warehouse where our source data resides and [Pachyderm](https://www.pachyderm.com/) as our data versioning and processing platform for non-SQL transformations. \n",
    "\n",
    "Specifically, this example shows how to integrate with and use data from a data warehouse to apply coding\n",
    "\n",
    "In it you'll learn how to:\n",
    "- Ingest data from Snowflake using the [Data Warehouse Integration](https://docs.pachyderm.com/latest/how-tos/basic-data-operations/sql-ingest/#data-warehouse-integration)\n",
    "- Transform your data to prepare it for model training\n",
    "- Train a churn model\n",
    "- Predict on new data with our churn model\n",
    "- Egress our predictions back to Snowflake using [Pachyderm's Egress Feature](https://docs.pachyderm.com/latest/how-tos/basic-data-operations/export-data-out-pachyderm/sql-egress/#egress-to-an-sql-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1dfde-fac3-4c73-b17a-1be6692ba50b",
   "metadata": {},
   "source": [
    "## 1. Install and Connect to Snowflake\n",
    "First, we need to set everything up for the example. You'll need: \n",
    "1. Snowflake account access (SYSADMIN role)\n",
    "2. [Pachyderm Cluster](https://docs.pachyderm.com/latest/deploy-manage/deploy/)\n",
    "3. [kubectl](https://kubernetes.io/docs/tasks/tools/) installed with Kubernetes access to where your Pachyderm cluster is running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe511b1c-ca32-488f-8395-59e18b7a37b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install snowflake-connector-python pyarrow\"<6.1.0,>=6.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c781f334-6f83-48ec-ac0e-ccb491a19617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc690cd3-a5e2-4f00-97d6-dda679eea0d9",
   "metadata": {},
   "source": [
    "For more information on setting these variables, [check here](https://docs.snowflake.com/en/user-guide/python-connector-install.html#step-2-verify-your-installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e90a0b-8b6f-4e2d-89c8-97a6d9575dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = '<your_username>'\n",
    "password = '<your_password>'\n",
    "account_identifier = '<your_account_id>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ab4a8-5472-4b50-9712-a8946c5c2ee0",
   "metadata": {},
   "source": [
    "First, we'll just check our configuration to make sure we set our snowflake information correctly by returning the current Snowflake version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0697e0-3270-4abd-9d77-f81a83738139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.19.0\n"
     ]
    }
   ],
   "source": [
    "ctx = snowflake.connector.connect(\n",
    "    user=user_name,\n",
    "    password=password,\n",
    "    account=account_identifier\n",
    "    )\n",
    "cs = ctx.cursor()\n",
    "try:\n",
    "    cs.execute(\"SELECT current_version()\")\n",
    "    one_row = cs.fetchone()\n",
    "    print(one_row[0])\n",
    "finally:\n",
    "    cs.close()\n",
    "# ctx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1cdd9-fed1-4842-930b-26ad6c6e207a",
   "metadata": {},
   "source": [
    "## 2: Setup Databases in Snowflake\n",
    "In the setup step, we'll create all the necessary componets for this example. By default we'll use the `COMPUTE_WH` that is common to Snowflake. The `SYSADMIN` role is also required for everything in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa371f90-9d36-49fc-87f0-6da9ecfae47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snowflake.connector.pandas_tools import write_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4869223-eae2-4735-a5b7-fa2cb4300afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_name = \"COMPUTE_WH\"\n",
    "database_name = \"KKBOX_CHURN_EXAMPLE\"\n",
    "schema_name = \"PUBLIC\"\n",
    "role = \"SYSADMIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f5cb5dd-3cd5-4fe1-8e73-b6ef1f49ca7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fee68c917c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create connector\n",
    "ctx = snowflake.connector.connect(\n",
    "    user=user_name,\n",
    "    password=password,\n",
    "    account=account_identifier,\n",
    "    client_session_keep_alive=True\n",
    "    )\n",
    "\n",
    "# Create a cursor object\n",
    "cur = ctx.cursor()\n",
    "\n",
    "# Define Setup properties for Snowflake\n",
    "\n",
    "# Starting with the Role.\n",
    "sql = \"USE ROLE {role}\".format(role=role)\n",
    "cur.execute(sql)\n",
    "\n",
    "# Specify the warehouse to use\n",
    "sql = \"USE WAREHOUSE {wh_name}\".format(wh_name = warehouse_name)\n",
    "cur.execute(sql)\n",
    "\n",
    "# See if the desired database exists.\n",
    "sql = \"CREATE DATABASE IF NOT EXISTS {db_name}\".format(db_name = database_name)\n",
    "cur.execute(sql)\n",
    "\n",
    "# And then use it.\n",
    "sql = \"USE DATABASE {db_name}\".format(db_name = database_name)\n",
    "cur.execute(sql)\n",
    "\n",
    "# Do the same with the Schema.\n",
    "sql = \"CREATE SCHEMA IF NOT EXISTS {sc_name}\".format(sc_name = schema_name)\n",
    "cur.execute(sql)\n",
    "\n",
    "# And then use it.\n",
    "sql = \"USE SCHEMA {sc_name}\".format(sc_name = schema_name)\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fa7b5-4c1b-4193-81f8-adc0838c0ce5",
   "metadata": {},
   "source": [
    "### 2.1 Create Kubernetes Secret to access Snowflake in Pachyderm\n",
    "To securely access our Snowflake data in Pachyderm, we set up a Kubernetes secret to store our passcode. Here' we're directly using [Kubernetes secrets](https://kubernetes.io/docs/concepts/configuration/secret/), but you could also use a [Pachyderm Secret](https://docs.pachyderm.com/latest/reference/pachctl/pachctl_create_secret/). \n",
    "\n",
    "Note: That you may have to modify the command if you're running Pachyderm in a specific Kubernetes namespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bf06a-5be1-475b-b97c-73224641a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must have kubectl and connect to your cluster\n",
    "!kubectl create secret generic snowflakesecret --type=string --from-literal=PACHYDERM_SQL_PASSWORD='<your_snowflake_password>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d7d93-bf4c-4cbe-b959-9f960d667c89",
   "metadata": {},
   "source": [
    "### 2.2 Create Snowflake Tables from CSV files\n",
    "This will simulate a production data warehouse in Snowflake. We will be able to run queries and add data to Snowflake just like a production application. But when it comes to performing ML or anything that required reproducibility and automation, Pachyderm will handle the data and processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a7586-fdf9-4bd6-8177-c6668e3b0e0c",
   "metadata": {},
   "source": [
    "**Download:** [kkbox-churn-prediction-challenge.zip](https://www.kaggle.com/competitions/kkbox-churn-prediction-challenge/data) from Kaggle. \n",
    "\n",
    "Note: You need to have a Kaggle account to download the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524837a8-9e8b-40be-831d-c3f6fff4c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data\n",
    "!unzip kkbox-churn-prediction-challenge.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5092dc12-d8df-4546-a615-8fc495386be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to convert pandas types to Snowflake data types\n",
    "def get_table_metadata(df):\n",
    "    def map_dtypes(x):\n",
    "        if (x == 'object') or (x=='category'):\n",
    "            return 'VARCHAR'\n",
    "        elif 'date' in x:\n",
    "            return 'DATETIME'\n",
    "        elif 'int' in x:\n",
    "            return 'NUMERIC'\n",
    "        elif 'float' in x:\n",
    "            return 'REAL'\n",
    "        elif 'bytes' in x:\n",
    "            return 'BINARY'\n",
    "        elif 'float' in x:\n",
    "            return 'REAL'\n",
    "        elif 'bytearray' in x:\n",
    "            return 'BINARY'\n",
    "        elif 'bool' in x:\n",
    "            return 'BOOLEAN'\n",
    "        else:\n",
    "            print(\"cannot parse pandas dtype\")\n",
    "    sf_dtypes = [map_dtypes(str(s)) for s in df.dtypes]\n",
    "    table_metadata = \", \". join([\" \".join([y.upper(), x]) for x, y in zip(sf_dtypes, list(df.columns))])\n",
    "    return table_metadata\n",
    "\n",
    "# Create/replace table with dataframe\n",
    "def df_to_snowflake_table(table_name, operation, df, conn=ctx): \n",
    "    if operation=='create_replace':\n",
    "        df.columns = [c.upper() for c in df.columns]\n",
    "        table_metadata = get_table_metadata(df)\n",
    "        conn.cursor().execute(f\"CREATE OR REPLACE TABLE {table_name} ({table_metadata})\")\n",
    "        write_pandas(conn, df, table_name.upper())\n",
    "    elif operation=='insert':\n",
    "        table_rows = str(list(df.itertuples(index=False, name=None))).replace('[','').replace(']','')\n",
    "        conn.cursor().execute(f\"INSERT INTO {table_name} VALUES {table_rows}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb0281-dea4-4b75-9060-86e7c63f0626",
   "metadata": {},
   "source": [
    "### 2.3 Table to Snowflake\n",
    "Now, we'll uncompress the data (using p7zip) and push it to Snowflake using pandas and the snowflake-connector-python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa330f2b-f4c5-45b5-8258-a76a6a364361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install p7zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359807c5-68a9-4c22-ae27-a82922c308a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!7zr -y x *.7z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c35557-ae5a-406d-95a3-3b9f96d3ecff",
   "metadata": {},
   "source": [
    "#### 2.3.1 Members table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498cc88f-f038-4bfb-a009-f8f87eff9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('members_v3.csv')\n",
    "\n",
    "print(members.dtypes)\n",
    "\n",
    "print('Pushing data (this may take a while)...')\n",
    "df_to_snowflake_table('MEMBERSHIPS', 'create_replace', members) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb08c2-91eb-42b1-8051-637abefb70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e8204-41ad-4207-a202-1915b1d69fa9",
   "metadata": {},
   "source": [
    "#### 2.3.2 Train table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a2b212-bcaa-4672-94ca-5b582267c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno        object\n",
      "is_churn     int64\n",
      "dtype: object\n",
      "Pushing data (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "print(train.dtypes)\n",
    "\n",
    "print('Pushing data (this may take a while)...')\n",
    "df_to_snowflake_table('TRAIN', 'create_replace', train) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ddd69c5-ed1e-466b-bc63-056680c3bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc7421-79b7-41dc-9362-20adb8c05603",
   "metadata": {},
   "source": [
    "#### 2.3.3 Transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4fe0d-0861-4868-94d5-b5470d51fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('transactions.csv')\n",
    "\n",
    "print(transactions.dtypes)\n",
    "\n",
    "print('Pushing data (this may take a while)...')\n",
    "df_to_snowflake_table('TRANSACTIONS', 'create_replace', transactions) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d66d1be-733f-418c-9df0-d15738493a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02113b16-04e1-476f-8466-015aec822b63",
   "metadata": {},
   "source": [
    "#### 2.3.4 User Logs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7ea17-9422-4a91-87ae-ffd144ddc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = pd.read_csv('user_logs.csv')\n",
    "\n",
    "print(user_logs.dtypes)\n",
    "\n",
    "print('Pushing data (this may take a while)...')\n",
    "df_to_snowflake_table('USER_LOGS', 'create_replace', user_logs) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0600fd-ce24-4674-803d-af71f144eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del user_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28b78e-75e3-4715-867d-408a14f15bb3",
   "metadata": {},
   "source": [
    "## 3. Import Churn Data From Snowflake to Pachyderm\n",
    "In this step we will use the Pachyderm Data Warehouse integration to ingress data. The pipelines will: \n",
    "\n",
    "1. Execute a SQL query\n",
    "2. Create a CSV file for the result\n",
    "3. Commit the CSV file to a Pachyderm versioned data repository\n",
    "\n",
    "The Data Warehouse integration will also allow us to run the ingress operation on a schedule. For our purposes in this example, we'll run every 24 hours and manually kick off jobs, but in a real world scenario, we would schedule ingestions specifically for our usecase. For example, we may only want to update our dataset once per day or once per week. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6cbc4-26aa-48c4-b7f8-b7399de3801f",
   "metadata": {},
   "source": [
    "### 3.1 Create Connection URL\n",
    "The data warehouse integration will parse a standardized URL to connect to our Snowflake instance. We'll automatically generate the URL using the details that were provided earlier in this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1642732-caca-4d38-a26c-9d4e3cf8d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowflake://jimmy@of28881.us-central1.gcp/KKBOX_CHURN_EXAMPLE/PUBLIC?warehouse=COMPUTE_WH\n"
     ]
    }
   ],
   "source": [
    "url = 'snowflake://{usr}@{account}/{database_name}/{schema_name}?warehouse={warehouse_name}'.format(usr = user_name,\n",
    "                                                                                              account = account_identifier,\n",
    "                                                                                              database_name = database_name,\n",
    "                                                                                              schema_name = schema_name,\n",
    "                                                                                              warehouse_name = warehouse_name)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45158371-5cb2-4e70-9bd7-85031f138d5c",
   "metadata": {},
   "source": [
    "### 3.2 Create SQL Query\n",
    "The querys that we execute in this example join features from the different sources to create our dataset. These joins are performed on the user's ID, which is our `msno` column.\n",
    "\n",
    "The structure of the queries are: \n",
    "\n",
    "**Training Data**:\n",
    "```sql\n",
    "select \n",
    "  distinct *\n",
    "from \n",
    "  CHURN_EXAMPLE.PUBLIC.USER_LOGS A \n",
    "  inner join CHURN_EXAMPLE.PUBLIC.TRAIN B sample (0.1) on A.msno = B.msno \n",
    "  and B.msno = A.msno \n",
    "  inner join CHURN_EXAMPLE.PUBLIC.TRANSACTIONS C on A.msno = C.msno \n",
    "  and C.msno = A.msno \n",
    "  inner join CHURN_EXAMPLE.PUBLIC.MEMBERSHIPS D on A.msno = D.msno \n",
    "  and D.msno = A.msno;\n",
    "```\n",
    "\n",
    "**Prediction Data**: \n",
    "\n",
    "```sql\n",
    "select \n",
    "  distinct * \n",
    "from \n",
    "  CHURN_EXAMPLE.PUBLIC.USER_LOGS A \n",
    "  inner join CHURN_EXAMPLE.PUBLIC.TRANSACTIONS C sample (1) on A.msno = C.msno \n",
    "  and C.msno = A.msno \n",
    "  inner join CHURN_EXAMPLE.PUBLIC.MEMBERSHIPS D on A.msno = D.msno \n",
    "  and D.msno = A.msno;\n",
    "```\n",
    "\n",
    "Both queries are essentially the same with the only difference being the presence of training labels in the training data. Which is a boolean value of whether or not that user churned. \n",
    "\n",
    "Note: In both queries, we explicitly define the column names we want to return due to the behavior of inner joins. We also add a `sample()` command to reduce the amount of data we query to make the example faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d468f4-75b4-4509-a6d0-2830d03d9ce4",
   "metadata": {},
   "source": [
    "### 3.3 Deploy the Ingestion Pipelines\n",
    "We can now use our configuration to query Snowflake and store our result in a Pachyderm repository. \n",
    "\n",
    "The integration uses [Jsonnet Pipeline Specifications](https://docs.pachyderm.com/latest/how-tos/pipeline-operations/jsonnet-pipeline-specs/#jsonnet-pipeline-specifications) to create 2 pipelines. The first one (`TRAIN_DATA_queries`) functions as a cron pipeline. It will run automatically according to the `cronSpec` we specify. When it runs, it writes our SQL query out to a file which triggers the second pipeline (`TRAIN_DATA`) which will run the query and store the result as a csv file.  \n",
    "\n",
    "\n",
    "<img src=\"images/data_warehouse_ingest.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Note: These queries will take a little while to run since the dataset is quite large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl update pipeline --jsonnet ./pachyderm/snowflake_import.jsonnet  \\\n",
    "    --arg name=TRAIN_DATA \\\n",
    "    --arg url=\"snowflake://jimmy@of28881.us-central1.gcp/KKBOX_CHURN_EXAMPLE/PUBLIC?warehouse=COMPUTE_WH\" \\\n",
    "    --arg query=\"select distinct A.MSNO, A.DATE, A.NUM_25, A.NUM_50, A.NUM_75, A.NUM_985, A.NUM_100, A.NUM_UNQ, A.TOTAL_SECS, B.IS_CHURN, C.PAYMENT_METHOD_ID, C.PAYMENT_PLAN_DAYS, C.PLAN_LIST_PRICE, C.ACTUAL_AMOUNT_PAID, C.IS_AUTO_RENEW, C.TRANSACTION_DATE, C.MEMBERSHIP_EXPIRE_DATE, C.IS_CANCEL, D.CITY, D.BD, D.GENDER, D.REGISTERED_VIA, D.REGISTRATION_INIT_TIME from CHURN_EXAMPLE.PUBLIC.USER_LOGS A inner join CHURN_EXAMPLE.PUBLIC.TRAIN B sample (0.1) on A.msno = B.msno and B.msno = A.msno inner join CHURN_EXAMPLE.PUBLIC.TRANSACTIONS C on A.msno = C.msno and C.msno = A.msno inner join CHURN_EXAMPLE.PUBLIC.MEMBERSHIPS D on A.msno = D.msno and D.msno = A.msno;\" \\\n",
    "    --arg cronSpec=\"@every 24h\" \\\n",
    "    --arg secretName=\"snowflakesecret\" \\\n",
    "    --arg format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1703870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force a cron tick to run the pipline initially (we don't want to wait 24 hours)\n",
    "!pachctl run cron TRAIN_DATA_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data table\n",
    "!pachctl update pipeline --jsonnet ./pachyderm/snowflake_import.jsonnet  \\\n",
    "    --arg name=PRED_DATA \\\n",
    "    --arg url=\"snowflake://jimmy@of28881.us-central1.gcp/KKBOX_CHURN_EXAMPLE/PUBLIC?warehouse=COMPUTE_WH\" \\\n",
    "    --arg query=\"select distinct A.MSNO, A.DATE, A.NUM_25, A.NUM_50, A.NUM_75, A.NUM_985, A.NUM_100, A.NUM_UNQ, A.TOTAL_SECS, C.PAYMENT_METHOD_ID, C.PAYMENT_PLAN_DAYS, C.PLAN_LIST_PRICE, C.ACTUAL_AMOUNT_PAID, C.IS_AUTO_RENEW, C.TRANSACTION_DATE, C.MEMBERSHIP_EXPIRE_DATE, C.IS_CANCEL, D.CITY, D.BD, D.GENDER, D.REGISTERED_VIA, D.REGISTRATION_INIT_TIME from CHURN_EXAMPLE.PUBLIC.USER_LOGS A inner join CHURN_EXAMPLE.PUBLIC.TRANSACTIONS C sample (1) on A.msno = C.msno and C.msno = A.msno inner join CHURN_EXAMPLE.PUBLIC.MEMBERSHIPS D on A.msno = D.msno and D.msno = A.msno;\" \\\n",
    "    --arg cronSpec=\"@every 24h\" \\\n",
    "    --arg secretName=\"snowflakesecret\" \\\n",
    "    --arg format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force a cron tick to run the pipline initially (we don't want to wait 24 hours)\n",
    "!pachctl run cron PRED_DATA_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878c9c8-b45d-4efb-8878-1e8e73edb15b",
   "metadata": {},
   "source": [
    "## 4. Process Data with Python and R\n",
    "Now that our data is in Pachyderm, we can run arbitrary processing on the data. This means that we can apply our python code to do data cleaning, feature engineering, model training, and anything else we desire through [Pachyderm pipelines](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/pipeline/). \n",
    "\n",
    "If we look at one of these files, we can see that we're calling python to process our csv data. Pachyderm passes the versioned data stored in data repositories as inputs to the file system of the container when it runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "880c339f-ba24-43b2-bfa4-99ff87e6691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pipeline\": {\n",
      "      \"name\": \"clean-data\"\n",
      "    },\n",
      "    \"description\": \"Clean and curate dataset.\",\n",
      "    \"input\": {\n",
      "      \"pfs\": {\n",
      "        \"repo\": \"TRAIN_DATA\",\n",
      "        \"glob\": \"/\"\n",
      "      }\n",
      "    },\n",
      "    \"transform\": {\n",
      "        \"cmd\": [\n",
      "          \"python\",\"/workdir/data-cleaning.py\",\"--data\",\"/pfs/TRAIN_DATA/0000\",\"--output\",\"/pfs/out/\"\n",
      "        ],\n",
      "      \"image\": \"jimmywhitaker/py_wsdm:dev0.18\"\n",
      "    }\n",
      "  }"
     ]
    }
   ],
   "source": [
    "!cat pachyderm/data-cleaning.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8ddf3-4f3e-4f31-9c87-82f1d3fc48fb",
   "metadata": {},
   "source": [
    "### 4.1 Create Pachyderm Pipelines\n",
    "Once created, our pipelines will automatically execute whenever our data changes, such as when we query new data from Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab1f5787-7b8c-402f-a56e-c49fbc946907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean our dataset\n",
    "!pachctl create pipeline -f pachyderm/clean-data.json\n",
    "\n",
    "# Create features from our cleaned data\n",
    "!pachctl create pipeline -f pachyderm/feature-engineering.json\n",
    "\n",
    "# Train a churn classification model\n",
    "!pachctl create pipeline -f pachyderm/model.json\n",
    "\n",
    "# Generate some data visualization for our tables\n",
    "# !pachctl create pipeline -f pachyderm/visualizations.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e62c8-5ce6-4ad2-8ebf-a7432b4b68a3",
   "metadata": {},
   "source": [
    "## 5. Predict Churn for Users\n",
    "Now that we have a trained model, we can apply it to our new data to figure out who is likely to churn according to the data we have on their usage of the KKBox service. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acc125-21ac-4bdf-a073-8de76653b784",
   "metadata": {},
   "source": [
    "### 5.1 Create a Snowflake Table to put our results\n",
    "We may want to do queries or other functions with our predicted churn data once we've computed it, so we'll egress this data from Pachyderm back to Snowflake. Before we do that, we'll create a table to put our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af9cb9d-00cf-44c1-98d6-6e194952dbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fee68c917c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same with the Schema.\n",
    "tbl_name = 'PREDICTIONS'\n",
    "sql = \"CREATE TABLE IF NOT EXISTS {tbl_name} (msno varchar (100), churn_prediction integer);\".format(tbl_name = tbl_name)\n",
    "cur.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa7a5c-2f84-417d-bb4b-5387a0738300",
   "metadata": {},
   "source": [
    "### 5.2 Create Pachyderm Pipelines\n",
    "Before we predict on our data, we'll clean and extract features to ensure our data is relevant to the model we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92054821-b0c6-434c-aed8-24deaa26f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data we're going to predict on\n",
    "!pachctl create pipeline -f pachyderm/clean-data-pred.json\n",
    "\n",
    "# Extract features from our prediction data\n",
    "!pachctl create pipeline -f pachyderm/feature-engineering-pred.json\n",
    "\n",
    "# Extract features from our prediction data\n",
    "!pachctl create pipeline -f pachyderm/predict.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4d61f-10c7-4161-84b5-088a8dc37f70",
   "metadata": {},
   "source": [
    "The final pipeline `predict` uses the Pachyderm [egress feature](https://docs.pachyderm.com/latest/how-tos/basic-data-operations/export-data-out-pachyderm/sql-egress/). This allows us to write data to Snowflake by configuring our pipeline specification with the appropriate information and Pachyderm handles everything for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "387c70a8-f9e4-4940-b159-aea0cc32975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pipeline\": {\n",
      "      \"name\": \"predict\"\n",
      "    },\n",
      "    \"description\": \"Predict churn probability for KKBox customers.\",\n",
      "    \"input\": {\n",
      "      \"cross\": [\n",
      "        {\n",
      "          \"pfs\": {\n",
      "            \"repo\": \"feature-engineering-pred\",\n",
      "            \"glob\": \"/\"\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"pfs\": {\n",
      "            \"repo\": \"model\",\n",
      "            \"glob\": \"/\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"transform\": {\n",
      "      \"cmd\": [\n",
      "        \"python\", \"predict.py\", \"--model\", \"/pfs/model/logistic_regression.sav\", \"--features\", \"/pfs/feature-engineering-pred/inference_features.csv\", \"--output\", \"/pfs/out/predictions/\"\n",
      "      ],\n",
      "      \"image\": \"jimmywhitaker/py_wsdm:dev0.19\"\n",
      "    },\n",
      "    \"egress\": {\n",
      "        \"sql_database\": {\n",
      "            \"url\": \"snowflake://jimmy@of28881.us-central1.gcp/KK_BOX_CHURN_EXAMPLE/PUBLIC?warehouse=COMPUTE_WH\",\n",
      "            \"file_format\": {\n",
      "                \"type\": \"CSV\",\n",
      "                \"columns\": [\"msno\", \"churn_prediction\"]\n",
      "            },\n",
      "            \"secret\": {\n",
      "                \"name\": \"snowflakesecret\",\n",
      "                \"key\": \"PACHYDERM_SQL_PASSWORD\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"resource_requests\": {\n",
      "      \"memory\": \"32G\",\n",
      "      \"cpu\": 8\n",
      "    }\n",
      "  }"
     ]
    }
   ],
   "source": [
    "!cat pachyderm/predict.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be0bf5-0910-44b5-8927-f71d27e28d97",
   "metadata": {},
   "source": [
    "## 6. View Results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9f3d1-bd63-4f86-b9cf-f462923eefea",
   "metadata": {},
   "source": [
    "We can now go to snowflake and view all the users that are at risk of churning on our service!\n",
    "\n",
    "<img src=\"images/churn_prediction.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e278813-6f0f-4743-bcd6-af4950de1b2d",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2d5d667-8673-405a-98c2-9d741eeebead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl delete pipeline --all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
